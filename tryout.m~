%% Example for Regression Discontinuity Tree
%   Agoston Reguly (2021)
clear all
clc

%% Generating simple RD
N  = 10000;
sigmaY = 1;
% Uniform running variable between -1 and 1
X = rand( [ N , 1 ] ) * 2 - 1;
% Threshold value
c = 0;
W = X >= c;
% Create two binary features
Z = rand( [ N , 2 ] );
Z( Z  > 0.5 ) = 1;
Z( Z <= 0.5 ) = 0;
% Two treatment effects, depends only on first feature
treatment =  1 .* double( Z( : , 1 ) == 0 ) + ...
             -1 .* double( Z( : , 1 ) >  0 );
% Conditional Expectation function is also heterogen
eta = 2 .* X .* Z( : , 2 ) - 2 .* X .* ( 1 - Z( : , 2 ) );
% Outcome
Y = eta + ( X >= c ) .* treatment + randn( [ N , 1 ] ) * sigmaY;
% Scatter plot to see the design
% scatter( X , Y )


%% CART options - Simple estimation
optTree                 = optCART;
optTree.maxNodes        = 50;
optTree.maxLevel        = 10;       % Maximum depth of the tree
optTree.maxLeaves       = 15;
optTree.minObs          = 50;       % Minimum observations within each leaf
optTree.cp              = 0.0001;   % Stopping rule for growing the tree: criterion must improve at least by this amount
optTree.maxIterGrow     = Inf;      % Maximum iteration during the growth of the tree
optTree.numKfold        = 5;        % number of K in K-fold cross-validation
optTree.CV1SE           = false;    % use of 1-SE rule in cross-validation
optTree.model           = 'linear'; % Set the model type: in case of RDD 'linear' is required
                                       % alternatively '
optTree.type            = 'CATE';   % CATE is for sharp RD 'CLATE' id for fuzzy design
optTree.criteria        = 'RDD';    % Use of RDD criterion, 
                                      % alternatively one can use 'athey-imbens' for experimental and observational studies with unconfoundedness 
                                      % or 'MSE' for rpart prediction case
optTree.varEstimator    = 'hce-1';  % type of variance estimator: 'simple','hce-0','hce-1'. 
                                      % For clustered SE, one needs to add the clusters when creating the sample
                                      % and use one of the estimators here as well
optTree.criteria_weight = 0.5;      % Criterion weight: 0.5 -> EMSE, using 1 -> `adaptive' criterion.
optTree.obsBucket       = 4;        % Minimum number of observations in the buckets
optTree.maxBucket       = Inf;      % Maximum number of buckets
optTree.orderPolinomial = p;        % Order of polinomial used during the estimation
setCritFnc( optTree );              % Set everything in the object of optTree


%% Set-up the sample object
% Indexes for the training sample and for the estimation sample:
%   here just simply split the sample in the middle, but can use any
%   sampling tecnique.
if mod( N / 2 , 2 ) == 0
    indTrEst = [ 1 : N / 2 ; N / 2 + 1 : N ]';
else
    indTrEst = [ 1 : ( N + 1 ) / 2 ; [ ( N + 1 ) / 2 + 1 : N , N ] ]';
end
% Set up the sample
obj_sample = samples( optTree , Y , Z , 'index4TrEst' , indTrEst , 'X' , X , 'cutoff' , c );
% can use 'cluster' and the variable to estimate clustered SE

%% Find and estimate the optimal tree
[ est_tree , large_tree , opt_gamma , cv_crit , cand_gamma , cv_all ] = runTree( obj_sample , optTree );
% Outputs:
%  est_tree: estimated optimal RD tree
% not necessary outputs:
%  large_tree: large tree estimated on training sample
%  opt_gamma: optimal pruning parameter
%  cv_crit: averaged cross-validation criteria for each candidate penalty parameters
%  cand_gamma: candidate penalty parameters
%  cv_all: cross-validation criteria for each candidate penalty parameters and for all folds

% Visualize the tree:
tostring( est_tree );



%% Finding optimal RD tree in details
%%%%%%%

% 1) Growing a large tree
[ obj_tree , stopWhy ] = growRDDtree( obj_sample , optTree );

% 2) Cross-validation:
%  - opt-beta: is the optimal penalty parameter
% if want to investigate the cross-validation function:
%  - s_oOS: is the average of the estimated cross-validation (out-of-sample)
%  criterion to check the curvature
%  - beta: candidate pruning parameters which are used
%  - oOS: all the K cross-validation criterion values

[ opt_beta , s_oOS , beta , oOS ] = cross_validate( obj_tree , obj_sample , optTree );

% 3) Prune with optimal penalty parameter:
%   gives the tree, estimated on the training sample!
finalTree_tr = pruning( obj_tree , obj_sample , optTree , opt_beta );

% 4) Run the estimation on the estimation sample
est_tree = estimate_tree_est( finalTree_tr , obj_sample , optTree );